#!/usr/bin/env python3
"""
è‡ªå‹•ç”ŸæˆåŒ…å«æƒ…ç·’ ID çš„ filelist
æ•´åˆ main_continue.py çš„æ¨™é»æ¸…æ´—èˆ‡ beji_cut éŸ³ç´ è½‰æ›é‚è¼¯
"""

import os
import sys
import glob
import argparse
import re
import random

# --- å¼·åˆ¶åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘ï¼Œè§£æ±º ImportError ---
cwd = os.getcwd()
if cwd not in sys.path:
    sys.path.append(cwd)

try:
    from tools.phonemes_transformation.zh.beji_cut_vowel_corpus import zh_frontend
except ImportError as e:
    print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥éŸ³ç´ è½‰æ›æ¨¡çµ„ã€‚è©³æƒ…: {e}")
    print(f"è«‹ç¢ºä¿æ‚¨æ˜¯åœ¨ vits å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œï¼špython3 tools/prepare_emotion_filelist.py")
    sys.exit(1)

# æƒ…ç·’æ˜ å°„
EMOTION_MAP = {
    'Neutral': 0, 'Happy': 1, 'Sad': 2, 'Angry': 3,
    'Surprise': 4, 'Fear': 5, 'Disgust': 6
}

class TextProcessor:
    """å®Œå…¨ç§»æ¤è‡ª main_continue.py çš„æ–‡æœ¬è™•ç†é‚è¼¯"""
    def __init__(self):
        self.beji_cut = zh_frontend()

    def to_half_width(self, text):
        converted_text = ''
        for char in text:
            ascii_code = ord(char)
            if 65281 <= ascii_code <= 65374:
                converted_char = chr(ascii_code - 65248)
            else:
                converted_char = char
            converted_text += converted_char
        return converted_text

    def contains_only_chinese_and_numbers(self, text):
        """æª¢æŸ¥æ˜¯å¦åªå«ä¸­æ–‡èˆ‡æ•¸å­— (ä¾†è‡ª main_continue.py)"""
        alpha = re.findall(r'[a-zA-Z]', text)
        return not bool(alpha)

    def content_punctuation(self, text):
        """æ¨™é»ç¬¦è™Ÿæ¸…æ´—èˆ‡æ¨™æº–åŒ– (ä¾†è‡ª main_continue.py)"""
        texted_process = self.to_half_width(text)
        texted_process = re.sub(r'(\W)\1+', r'\1 ', texted_process)
        texted_process = texted_process.replace('\\n', 'ã€‚')

        # ç§»é™¤ ESD å¸¸è¦‹çš„ç‰¹æ®Šç¬¦è™Ÿ
        texted_process = texted_process.replace('â€¦', '').replace('...', '')
        # ç§»é™¤æˆ–æ›¿æ›é “è™Ÿ â†’ é “è™Ÿæœƒè¢« beji_cut èª¤è½‰æˆ "ã€1" å’Œ "1ã€" éŸ³ç´ 
        texted_process = texted_process.replace('ã€', 'ï¼Œ')  # æ›¿æ›ç‚ºé€—è™Ÿ

        # ç§»é™¤å„ç¨®ç ´æŠ˜è™Ÿ (æœƒè¢« beji_cut èª¤è½‰æˆ "â€•1" "1â€•" "â€”1" "1â€”" ç­‰éŸ³ç´ )
        # U+2014 (EM DASH): â€”, U+2015 (HORIZONTAL BAR): â€•
        for dash in ['â€•â€•', 'â€”â€”', 'â€•', 'â€”', '--']:
            texted_process = texted_process.replace(dash, 'ï¼Œ')

        for char in 'ã€ã€ã€Œã€Œã€ã€ã€Šã€‹()[]ï¼ˆï¼‰':
            texted_process = texted_process.replace(char, "")

        texted_process = texted_process.strip()
        # ç¢ºä¿æœ‰çµå°¾æ¨™é»
        if texted_process and not texted_process.endswith(('ã€‚', '!', '?', 'ï¼', 'ï¼Ÿ')):
            texted_process += 'ã€‚'

        return texted_process

    def get_phonemes(self, text):
        # 1. éæ¿¾åŒ…å«è‹±æ–‡çš„å¥å­
        if not self.contains_only_chinese_and_numbers(text):
            return None
            
        # 2. æ¨™é»ç¬¦è™Ÿè™•ç†
        cleaned_text = self.content_punctuation(text)
        if len(cleaned_text) < 3: # åƒè€ƒ main_continue é‚è¼¯
            return None
            
        # 3. éŸ³ç´ è½‰æ› (beji_cut)
        result, status = self.beji_cut.get_phonemes(cleaned_text)
        return result if status else None

def generate_filelist(data_root, output_file, language='ZH', train_ratio=0.9):
    tp = TextProcessor()
    train_lines = []
    val_lines = []
    file_count = 0

    print(f"Scanning dataset at: {data_root}")

    speakers = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])

    for speaker in speakers:
        speaker_path = os.path.join(data_root, speaker)
        ids = re.findall(r'\d+', speaker)
        speaker_id = int(ids[-1]) if ids else hash(speaker) % 1000

        emotion_folders = sorted([d for d in os.listdir(speaker_path) if os.path.isdir(os.path.join(speaker_path, d))])

        for folder in emotion_folders:
            eid = next((i for name, i in EMOTION_MAP.items() if name.upper() in folder.upper()), None)
            if eid is None: continue

            wav_files = sorted(glob.glob(os.path.join(speaker_path, folder, "*.wav")))
            # åŠ å…¥éš¨æ©Ÿæ‰“æ•£ï¼Œç¢ºä¿ train/val åˆ†å¸ƒå‡å‹»
            random.shuffle(wav_files)

            for i, wav_path in enumerate(wav_files):
                txt_path = wav_path.replace('.wav', '.txt')
                if not os.path.exists(txt_path): continue

                with open(txt_path, 'r', encoding='utf-8') as tf:
                    raw_text = tf.read().strip()

                # åŸ·è¡Œèˆ‡ main_continue ä¸€è‡´çš„è™•ç†
                phonemes = tp.get_phonemes(raw_text)
                
                if phonemes:
                    line = f"{wav_path}|{speaker_id}|{language}|{phonemes}|{eid}\n"
                    if i < len(wav_files) * train_ratio:
                        train_lines.append(line)
                    else:
                        val_lines.append(line)
                    file_count += 1

    # å¯«å…¥çµæœ
    with open(f"{output_file}_train.txt", 'w', encoding='utf-8') as f:
        f.writelines(train_lines)
    with open(f"{output_file}_val.txt", 'w', encoding='utf-8') as f:
        f.writelines(val_lines)

    print(f"\nâœ… æˆåŠŸç”¢ç”Ÿ Filelistï¼ç¸½ç­†æ•¸: {file_count}")
    print(f"ğŸ“Š æ•¸æ“šåˆ†å¸ƒ:")
    for name, eid in sorted(EMOTION_MAP.items(), key=lambda x: x[1]):
        count = sum(1 for l in train_lines+val_lines if l.rstrip().endswith(f"|{eid}"))
        if count > 0: print(f"   {name:10}: {count} ç­†")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, required=True)
    parser.add_argument('--output', type=str, default='dataset/emotion')
    parser.add_argument('--language', type=str, default='ZH')
    parser.add_argument('--train_ratio', type=float, default=0.9)
    args = parser.parse_args()
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    generate_filelist(args.data_root, args.output, args.language, args.train_ratio)