# VITS with Emotion Control - å®Œæ•´è«–æ–‡æ¶æ§‹åœ–

> ğŸ“Š åŸºæ–¼å¯¦éš›ç¨‹å¼ç¢¼çš„æ¶æ§‹åˆ†æ
> ğŸ¯ åŒ…å«: eGeMAPS, CLN (Conditional Layer Normalization), CCA (Cross-Conditional Attention)
> ğŸ“… ç”Ÿæˆæ—¥æœŸ: 2025-12-31

---

## ğŸ¨ å®Œæ•´ç³»çµ±æ¶æ§‹åœ–

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 VITS with eGeMAPS + CLN + CCA Architecture                   â•‘
â•‘                     (åŸºæ–¼å¯¦éš› models.py å¯¦ä½œ)                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              TRAINING PHASE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¼¸å…¥è³‡æ–™:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text/Phoneme â”‚ â”‚  Speaker ID  â”‚ â”‚  Emotion ID  â”‚ â”‚  Reference Audio     â”‚
â”‚  [B, T_text] â”‚ â”‚     [B]      â”‚ â”‚     [B]      â”‚ â”‚   [B, T_wav]         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚                       â”‚
       â”‚                â”‚                â”‚                       â”‚
       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”‚
       â”‚         â”‚   Speaker & Emotion Fusion   â”‚                â”‚
       â”‚         â”‚  (models.py:527-536)         â”‚                â”‚
       â”‚         â”‚                               â”‚                â”‚
       â”‚         â”‚  g = emb_g(sid).unsqueeze(-1) â”‚                â”‚
       â”‚         â”‚  if n_emotions > 0:           â”‚                â”‚
       â”‚         â”‚    g_e = emb_e(eid).unsqueezeâ”‚                â”‚
       â”‚         â”‚    g = g + g_e   â† å‘é‡ç›¸åŠ    â”‚                â”‚
       â”‚         â”‚  [B, gin_ch, 1]               â”‚                â”‚
       â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
       â”‚                â”‚                                         â”‚
       â”‚                â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                   â”‚
       â”‚                â”‚                   â–¼
       â”‚                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚      â”‚ âœ¨ eGeMAPS Extractor (NEW)               â”‚
       â”‚                â”‚      â”‚ (egemaps_extractor.py:20-229)            â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Input: Reference Audio [B, T_wav]        â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Step 1: Mel-Spectrogram                 â”‚
       â”‚                â”‚      â”‚   mel_spec = MelSpec(audio)              â”‚
       â”‚                â”‚      â”‚   [B, 80, T_frames]                      â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Step 2: Extract Features                â”‚
       â”‚                â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚                â”‚      â”‚   â”‚ F0 (Pitch):                 â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Autocorrelation method   â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Range: [80, 600] Hz      â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Shape: [B, T_frames]     â”‚       â”‚
       â”‚                â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       â”‚                â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚                â”‚      â”‚   â”‚ Energy (RMS):               â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - âˆš(mean(mel_specÂ²))       â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Shape: [B, T_frames]     â”‚       â”‚
       â”‚                â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       â”‚                â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚                â”‚      â”‚   â”‚ Spectral Flux:              â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - âˆš(mean(diffÂ²))           â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Shape: [B, T_frames]     â”‚       â”‚
       â”‚                â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       â”‚                â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚                â”‚      â”‚   â”‚ Zero Crossing Rate:         â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Sign changes per frame   â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Shape: [B, T_frames]     â”‚       â”‚
       â”‚                â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       â”‚                â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
       â”‚                â”‚      â”‚   â”‚ MFCC (13-dim):              â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Mel-frequency cepstral   â”‚       â”‚
       â”‚                â”‚      â”‚   â”‚  - Shape: [B, 13, T_frames] â”‚       â”‚
       â”‚                â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Step 3: Concatenate                     â”‚
       â”‚                â”‚      â”‚   all_feat = [MFCC, Mel, Prosody]       â”‚
       â”‚                â”‚      â”‚   [B, 13+80+4, T_frames]                â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Step 4: Learnable Projection            â”‚
       â”‚                â”‚      â”‚   Linear(97 â†’ 256 â†’ 88)                 â”‚
       â”‚                â”‚      â”‚   egemaps_feat = [B, 88, T_feat]        â”‚
       â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                  â”‚
       â”‚                â”‚                  â–¼
       â”‚                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚      â”‚ âœ¨ eGeMAPS Encoder (NEW)                 â”‚
       â”‚                â”‚      â”‚ (egemaps_extractor.py:231-317)           â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Input: egemaps_feat [B, 88, T_feat]      â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Architecture:                            â”‚
       â”‚                â”‚      â”‚   pre = Conv1d(88 â†’ 192)                â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚   For i in [1, 2, 3]:                    â”‚
       â”‚                â”‚      â”‚     conv = Conv1d(192 â†’ 192, k=5)       â”‚
       â”‚                â”‚      â”‚     norm = LayerNorm(192)                â”‚
       â”‚                â”‚      â”‚     x = ReLU(norm(conv(x)))              â”‚
       â”‚                â”‚      â”‚     x = x + residual  â† æ®˜å·®é€£æ¥          â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚   proj = Conv1d(192 â†’ 192)              â”‚
       â”‚                â”‚      â”‚                                          â”‚
       â”‚                â”‚      â”‚ Output: emo_feat [B, 192, T_feat]        â”‚
       â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                  â”‚ emo_feat
       â”‚                â”‚                  â”‚ [B, hidden_ch, T_feat]
       â”‚                â”‚                  â”‚
       â–¼                â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   âœ¨ TEXT ENCODER with CLN & CCA                            â”‚
â”‚                      (models.py:137-201)                                    â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Phoneme Embedding                                               â”‚    â”‚
â”‚  â”‚    x = Embedding(phoneme_ids) * âˆšhidden_ch                         â”‚    â”‚
â”‚  â”‚    [B, T_text] â†’ [B, hidden_ch, T_text]                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                         â”‚
â”‚                                  â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. Transformer Encoder (Ã—6 layers)                                 â”‚    â”‚
â”‚  â”‚    (attentions.py:13-47)                                           â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚    For each layer i:                                               â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚      â”‚ (a) Multi-Head Self-Attention                        â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     - n_heads = 2                                    â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     - Relative Position Encoding                     â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     y = MultiHeadAttn(x, x, attn_mask)              â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     y = Dropout(y)                                   â”‚     â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                     â–¼                                              â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚      â”‚ âœ¨ (b) Conditional Layer Norm (CLN) - NEW           â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     (modules.py:34-75, attentions.py:31)            â”‚     â”‚    â”‚
â”‚  â”‚      â”‚                                                      â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     x_norm = LayerNorm(x + y)                       â”‚     â”‚    â”‚
â”‚  â”‚      â”‚                                                      â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     if g is not None:  â† Speaker/Emotion condition  â”‚     â”‚    â”‚
â”‚  â”‚      â”‚       gamma_c, beta_c = Conv1d(g).split()           â”‚     â”‚    â”‚
â”‚  â”‚      â”‚       output = x_norm * (1 + gamma_c) + beta_c      â”‚     â”‚    â”‚
â”‚  â”‚      â”‚                â†‘                                     â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     å‹•æ…‹èª¿æ•´æ¯å€‹ speaker/emotion çš„ç‰¹å¾µåˆ†ä½ˆ            â”‚     â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                     â–¼                                              â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚      â”‚ (c) Feed-Forward Network                            â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     y = FFN(x, x_mask)                              â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     y = Dropout(y)                                   â”‚     â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                     â–¼                                              â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚      â”‚ âœ¨ (d) Conditional Layer Norm (CLN) - NEW           â”‚     â”‚    â”‚
â”‚  â”‚      â”‚     x = CLN(x + y, g=g)                             â”‚     â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚    End for                                                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚ x_encoded                               â”‚
â”‚                                  â”‚ [B, hidden_ch, T_text]                  â”‚
â”‚                                  â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ âœ¨ 3. Cross-Conditional Attention (CCA) - NEW                      â”‚    â”‚
â”‚  â”‚    (attentions.py:257-345, models.py:193-196)                      â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚    if use_cca and emo_feat is not None:                           â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚
â”‚  â”‚      â”‚ Query:  Q = Conv_q(x_encoded)                       â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         â†‘ ä¾†è‡ªæ–‡æœ¬ç·¨ç¢¼                                â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         [B, hidden_ch, T_text]                      â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚ Key:    K = Conv_k(emo_feat)                        â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         â†‘ ä¾†è‡ª eGeMAPS emotion features             â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         [B, hidden_ch, T_feat]                      â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚ Value:  V = Conv_v(emo_feat)                        â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         â†‘ ä¾†è‡ª eGeMAPS emotion features             â”‚      â”‚    â”‚
â”‚  â”‚      â”‚         [B, hidden_ch, T_feat]                      â”‚      â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚
â”‚  â”‚                          â”‚                                         â”‚    â”‚
â”‚  â”‚                          â–¼                                         â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚
â”‚  â”‚      â”‚ Multi-Head Cross-Attention:                         â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   Q, K, V reshape to [B, n_heads, T, d_k]          â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   scores = (Q @ K^T) / âˆšd_k                         â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   scores.shape = [B, n_heads, T_text, T_feat]      â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   if mask provided:                                 â”‚      â”‚    â”‚
â”‚  â”‚      â”‚     attn_mask = x_mask Ã— emo_mask                   â”‚      â”‚    â”‚
â”‚  â”‚      â”‚     scores = masked_fill(scores, -1e4)             â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   attn_weights = Softmax(scores, dim=-1)           â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   x_cca = attn_weights @ V                          â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   [B, n_heads, T_text, d_k]                        â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   x_cca = Conv_o(x_cca)  â† output projection       â”‚      â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚
â”‚  â”‚                          â”‚                                         â”‚    â”‚
â”‚  â”‚                          â–¼                                         â”‚    â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚
â”‚  â”‚      â”‚ Residual Connection + Layer Norm:                   â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   x = LayerNorm(x_encoded + x_cca)                 â”‚      â”‚    â”‚
â”‚  â”‚      â”‚       â†‘                      â†‘                      â”‚      â”‚    â”‚
â”‚  â”‚      â”‚    åŸå§‹æ–‡æœ¬             æƒ…æ„Ÿè³‡è¨Š                      â”‚      â”‚    â”‚
â”‚  â”‚      â”‚                                                     â”‚      â”‚    â”‚
â”‚  â”‚      â”‚   ä¿ç•™æ–‡æœ¬å…§å®¹ï¼ŒåŒæ™‚æ³¨å…¥æƒ…æ„Ÿè¡¨ç¾åŠ›                     â”‚      â”‚    â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                         â”‚
â”‚                                  â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. Prior Distribution Projection                                   â”‚    â”‚
â”‚  â”‚    stats = Conv1d(x, out_channels*2)                               â”‚    â”‚
â”‚  â”‚    m_p, logs_p = split(stats)  â† Prior mean & log-variance         â”‚    â”‚
â”‚  â”‚    [B, inter_ch, T_text]                                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ m_p, logs_p (Prior Distribution)
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Monotonic Alignment   â”‚
              â”‚ Search (MAS)          â”‚
              â”‚ (monotonic_align/)    â”‚
              â”‚                       â”‚
              â”‚ è‡ªå‹•å°é½Šæ–‡æœ¬å’ŒéŸ³è¨Š       â”‚
              â”‚ ç„¡éœ€äººå·¥æ¨™è¨»            â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ attn [B, 1, T_y, T_x]
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ POSTERIOR ENCODER     â”‚         â”‚   DURATION PREDICTORS       â”‚      â”‚
â”‚  â”‚ (models.py:237-266)   â”‚         â”‚                             â”‚      â”‚
â”‚  â”‚ Training Only         â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚                       â”‚         â”‚ â”‚ âœ¨ Stochastic (SDP)     â”‚ â”‚      â”‚
â”‚  â”‚ Input: Mel-Spec       â”‚         â”‚ â”‚ (models.py:19-97)       â”‚ â”‚      â”‚
â”‚  â”‚ [B, n_mel, T_y]       â”‚         â”‚ â”‚                         â”‚ â”‚      â”‚
â”‚  â”‚        â”‚              â”‚         â”‚ â”‚ - Flow-based (Ã—4)       â”‚ â”‚      â”‚
â”‚  â”‚        â–¼              â”‚         â”‚ â”‚ - DDSConv processing    â”‚ â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚ â”‚ - Adds variation        â”‚ â”‚      â”‚
â”‚  â”‚ â”‚ Pre Conv1d  â”‚       â”‚         â”‚ â”‚ - logw_sdp              â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚        â–¼              â”‚         â”‚         â”‚                   â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚         â–¼                   â”‚      â”‚
â”‚  â”‚ â”‚ WaveNet     â”‚       â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚ (16 layers) â”‚       â”‚         â”‚ â”‚ âœ¨ Deterministic (DP)   â”‚ â”‚      â”‚
â”‚  â”‚ â”‚ + Cond on g â”‚       â”‚         â”‚ â”‚ (models.py:100-134)     â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚ â”‚                         â”‚ â”‚      â”‚
â”‚  â”‚        â–¼              â”‚         â”‚ â”‚ Conv + CLN (Ã—2) â† NEW  â”‚ â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚ â”‚ - Stable baseline       â”‚ â”‚      â”‚
â”‚  â”‚ â”‚ Proj Conv1d â”‚       â”‚         â”‚ â”‚ - logw_dp               â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚        â”‚              â”‚         â”‚         â”‚                   â”‚      â”‚
â”‚  â”‚        â–¼              â”‚         â”‚         â–¼                   â”‚      â”‚
â”‚  â”‚ m_q, logs_q, z        â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ [B, inter_ch, T_y]    â”‚         â”‚ â”‚ Mix Predictions:        â”‚ â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚                         â”‚ â”‚      â”‚
â”‚           â”‚ Posterior              â”‚ â”‚ logw = sdp_ratio * sdp  â”‚ â”‚      â”‚
â”‚           â”‚                        â”‚ â”‚      + (1-ratio) * dp   â”‚ â”‚      â”‚
â”‚           â–¼                        â”‚ â”‚                         â”‚ â”‚      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚ sdp_ratio = 0.1 (def)  â”‚ â”‚      â”‚
â”‚  â”‚ Residual         â”‚              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚ Coupling Block   â”‚              â”‚                             â”‚      â”‚
â”‚  â”‚ (Flow Ã—4)        â”‚              â”‚ Duration Losses:            â”‚      â”‚
â”‚  â”‚ (models.py:204)  â”‚              â”‚   L_sdp + L_dp              â”‚      â”‚
â”‚  â”‚                  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”‚ z_p = Flow(z, g) â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚           â”‚ z_p                                                         â”‚
â”‚           â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                 KL Divergence Loss                          â”‚       â”‚
â”‚  â”‚  L_kl = KL(q(z|y) || p(z|x))                               â”‚       â”‚
â”‚  â”‚       = KL(Posterior || Prior)                             â”‚       â”‚
â”‚  â”‚       Forces prior (text) to match posterior (audio)        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  GENERATOR (Decoder) â”‚
              â”‚  (models.py:269-321) â”‚
              â”‚  HiFi-GAN based      â”‚
              â”‚                      â”‚
              â”‚ Input: z_slice       â”‚
              â”‚ [B, inter_ch, seg]   â”‚
              â”‚         â”‚            â”‚
              â”‚         â–¼            â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚ â”‚ Pre Conv (7Ã—7) â”‚   â”‚
              â”‚ â”‚ + Cond on g    â”‚   â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚          â–¼            â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚ â”‚ Upsamples Ã—4   â”‚   â”‚
              â”‚ â”‚ rates: config  â”‚   â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚          â–¼            â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚ â”‚ ResBlocks      â”‚   â”‚
              â”‚ â”‚ (multi-kernel) â”‚   â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚          â–¼            â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚ â”‚ Post Conv+Tanh â”‚   â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚          â”‚            â”‚
              â”‚          â–¼            â”‚
              â”‚    y_hat (waveform)  â”‚
              â”‚    [B, 1, T_wav]     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Multi-Period Discriminator       â”‚
         â”‚   (models.py:389-411)              â”‚
         â”‚                                    â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Period Discriminators (Ã—5)   â”‚  â”‚
         â”‚  â”‚ Periods: 2, 3, 5, 7, 11      â”‚  â”‚
         â”‚  â”‚ (models.py:324-358)          â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Scale Discriminator (Ã—1)     â”‚  â”‚
         â”‚  â”‚ (models.py:361-386)          â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â”‚                                    â”‚
         â”‚  Outputs: D(y_real), D(y_hat)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LOSS FUNCTIONS                                    â”‚
â”‚                         (losses.py)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Generator Losses:
  1. L_adv   = generator_loss(disc_outputs)
  2. L_fm    = feature_loss(fmap_r, fmap_g)
  3. L_mel   = L1(mel(y), mel(y_hat)) Ã— c_mel (45)
  4. L_kl    = kl_loss(z_p, logs_q, m_p, logs_p) Ã— c_kl (1.0)
  5. L_dur   = l_length_sdp + l_length_dp

  L_G_total = L_adv + L_fm + L_mel + L_kl + L_dur

Discriminator Loss:
  L_D = discriminator_loss(y_d_hat_r, y_d_hat_g)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INFERENCE PHASE                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Text   â”‚ â”‚Speaker IDâ”‚ â”‚Emotion IDâ”‚ â”‚ Reference    â”‚
â”‚          â”‚ â”‚  (opt)   â”‚ â”‚  (opt)   â”‚ â”‚ Audio (opt)  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚             â”‚               â”‚
     â”‚            â–¼             â–¼               â”‚
     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
     â”‚     â”‚ g = emb_g(sid)       â”‚             â”‚
     â”‚     â”‚   + emb_e(eid)       â”‚             â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
     â”‚                â”‚                         â”‚
     â”‚                â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                â”‚              â”‚ eGeMAPS Extractor â”‚
     â”‚                â”‚              â”‚      â†“            â”‚
     â”‚                â”‚              â”‚ eGeMAPS Encoder   â”‚
     â”‚                â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚                         â”‚ emo_feat
     â–¼                â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text â†’ TextEncoder(with CCA if emo_feat)             â”‚
â”‚       â†’ m_p, logs_p                                   â”‚
â”‚       â†’ Duration: logw = 0.1*SDP + 0.9*DP            â”‚
â”‚       â†’ w_ceil = ceil(exp(logw))                      â”‚
â”‚       â†’ Expand to audio length via alignment          â”‚
â”‚       â†’ z_p = m_p + randn() * exp(logs_p) * 0.6      â”‚
â”‚       â†’ z = Flow^-1(z_p, g)                          â”‚
â”‚       â†’ waveform = Generator(z, g)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Inference Hyperparameters (models.py:674-675):
  - noise_scale: 0.6 (controls variation)
  - noise_scale_w: 0.2 (duration variation)
  - length_scale: 1.0 (speech rate)
  - sdp_ratio: 0.1 (SDP vs DP mix, line 731)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   KEY INNOVATIONS (æ‚¨çš„å‰µæ–°é»)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. âœ¨ eGeMAPS Feature Extraction
   ä½ç½®: egemaps_extractor.py:20-229
   åŠŸèƒ½:
   - å¾åƒè€ƒéŸ³è¨Šæå–è²å­¸ç‰¹å¾µ
   - F0, Energy, MFCC (13), Spectral Flux, ZCR, Mel-Spec (80)
   - ç¸½å…± 97 ç¶­ â†’ æŠ•å½±åˆ° 88 ç¶­
   - å¯å­¸ç¿’çš„æŠ•å½±å±¤ (Linear 97â†’256â†’88)

   å„ªå‹¢:
   - ç´°ç²’åº¦æƒ…æ„Ÿæ§åˆ¶
   - æ˜ç¢ºçš„è²å­¸ç‰¹å¾µå»ºæ¨¡
   - å¯è§£é‡‹æ€§å¼·

2. âœ¨ eGeMAPS Encoder
   ä½ç½®: egemaps_extractor.py:231-317, models.py:478-482
   åŠŸèƒ½:
   - å°‡ 88 ç¶­ eGeMAPS ç·¨ç¢¼ç‚º 192 ç¶­éš±ç©ºé–“
   - 3å±¤ Conv1d + LayerNorm + ReLU + Residual
   - è¼¸å‡ºèˆ‡ TextEncoder ç¶­åº¦å°é½Š

   å‰µæ–°:
   - æ®˜å·®é€£æ¥ä¿ç•™åŸå§‹ç‰¹å¾µ
   - é©æ‡‰æ€§å¼· (å¯è™•ç†è®Šé•·åºåˆ—)

3. âœ¨ Cross-Conditional Attention (CCA)
   ä½ç½®: attentions.py:257-345, models.py:174-180, 193-196
   æ©Ÿåˆ¶:
   - Query from Text Encoding
   - Key/Value from eGeMAPS Emotion Features
   - Multi-head cross-attention (n_heads=2)
   - Residual: x_out = x + CCA(x, emo_feat)

   å„ªå‹¢:
   - æ–‡æœ¬å¯ä»¥ã€ŒæŸ¥è©¢ã€éœ€è¦çš„æƒ…æ„Ÿç‰¹å¾µ
   - æ¯” concatenate æ›´éˆæ´»
   - æ¯” element-wise add æ›´å…·é¸æ“‡æ€§

4. âœ¨ Conditional Layer Normalization (CLN)
   ä½ç½®: modules.py:34-75
   å…¬å¼:
   ```
   x_norm = LayerNorm(x)
   if g is not None:
     gamma_c, beta_c = Conv1d(g).split(2)
     output = x_norm * (1 + gamma_c) + beta_c
   else:
     output = x_norm
   ```

   æ‡‰ç”¨ä½ç½®:
   - TextEncoder çš„ Transformer æ¯å±¤ (attentions.py:31,33)
   - DurationPredictor (models.py:112,114)
   - Decoder (attentions.py:71,73,75)

   å„ªå‹¢:
   - å‹•æ…‹èª¿æ•´ speaker/emotion çš„ç‰¹å¾µåˆ†ä½ˆ
   - ç›¸æ¯” FiLM æ›´è¼•é‡
   - è¨“ç·´ç©©å®š (åˆå§‹åŒ–ç‚º0)

5. ğŸ­ Dual Duration Prediction
   ä½ç½®: models.py:503-505 (SDP), models.py:505 (DP)
   æ··åˆç­–ç•¥ (models.py:731-736):
   ```python
   logw_sdp = self.sdp(x, x_mask, g=g, reverse=True)
   logw_dp  = self.dp(x, x_mask, g=g)
   logw = 0.1 * logw_sdp + 0.9 * logw_dp
   ```

   å„ªå‹¢:
   - SDP: æ·»åŠ éŸ»å¾‹è®ŠåŒ–
   - DP: æä¾›ç©©å®šåŸºç·š
   - æ··åˆ: å¹³è¡¡è‡ªç„¶åº¦å’Œå¯æ§æ€§

6. ğŸ¤ Multi-Speaker & Multi-Emotion Support
   ä½ç½®: models.py:507-510, 527-536
   èåˆæ–¹å¼:
   ```python
   g = emb_g(sid).unsqueeze(-1)  # Speaker
   if n_emotions > 0:
     g_e = emb_e(eid).unsqueeze(-1)  # Emotion
     g = g + g_e  # å‘é‡ç›¸åŠ èåˆ
   ```

   æ‡‰ç”¨:
   - å‚³å…¥æ‰€æœ‰æ¢ä»¶æ¨¡çµ„ (Encoder, DP, Decoder)
   - é€é CLN å‹•æ…‹èª¿æ•´ç‰¹å¾µ


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL SPECIFICATIONS                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Architecture (from code):
  - n_vocab: è®Šå‹• (depend on language symbols)
  - hidden_channels: 192
  - inter_channels: 192
  - filter_channels: 768
  - n_heads: 2
  - n_layers: 6 (Transformer)
  - n_speakers: 58 (models.py:464)
  - n_emotions: å¯é…ç½® (models.py:465)
  - use_sdp: True
  - use_cca: å¯é…ç½® (models.py:441)
  - use_egemaps: å¯é…ç½® (models.py:442)
  - eGeMAPS feature_dim: 88 (models.py:443)

Audio Processing:
  - sample_rate: 22050 Hz (models.py:444)
  - n_fft: 1024
  - hop_length: 256
  - win_length: 1024
  - n_mel_channels: 80

Training (typical config):
  - batch_size: 64
  - segment_size: 4096 frames
  - learning_rate: 2e-4
  - fp16_run: True
  - c_mel: 45
  - c_kl: 1.0


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TENSOR SHAPE FLOW (è¨“ç·´éšæ®µ)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Inputs:
   text:       [B, T_text]
   sid:        [B]
   eid:        [B]  (optional)
   ref_audio:  [B, T_wav]
   spec:       [B, n_mel=80, T_spec]

2. Speaker/Emotion Embeddings:
   emb_g(sid):    [B, gin_ch] â†’ unsqueeze(-1) â†’ [B, gin_ch, 1]
   emb_e(eid):    [B, gin_ch] â†’ unsqueeze(-1) â†’ [B, gin_ch, 1]
   g = emb_g + emb_e:  [B, gin_ch, 1]

3. eGeMAPS Processing:
   ref_audio [B, T_wav]
     â†’ egemaps_extractor â†’ [B, 88, T_feat]
     â†’ egemaps_encoder   â†’ [B, 192, T_feat]  (emo_feat)

4. Text Encoding:
   text [B, T_text]
     â†’ Embedding â†’ [B, T_text, 192]
     â†’ transpose â†’ [B, 192, T_text]
     â†’ Transformer (Ã—6 with CLN) â†’ [B, 192, T_text]
     â†’ CCA(x, emo_feat) if use_cca â†’ [B, 192, T_text]
     â†’ proj â†’ [B, 384, T_text]
     â†’ split â†’ m_p [B, 192, T_text], logs_p [B, 192, T_text]

5. Posterior Encoding:
   spec [B, 80, T_spec]
     â†’ pre â†’ [B, 192, T_spec]
     â†’ WN  â†’ [B, 192, T_spec]
     â†’ proj â†’ [B, 384, T_spec]
     â†’ split â†’ m_q [B, 192, T_spec], logs_q [B, 192, T_spec]
     â†’ sample â†’ z [B, 192, T_spec]

6. Flow:
   z [B, 192, T_spec]
     â†’ Flow(z, g) â†’ z_p [B, 192, T_spec]

7. Alignment:
   MAS(z_p, m_p, logs_p)
     â†’ attn [B, 1, T_spec, T_text]
     â†’ w = attn.sum(2) [B, 1, T_text]

8. Duration Prediction:
   SDP: logw_sdp [B, 1, T_text]
   DP:  logw_dp  [B, 1, T_text]
   Mix: logw = 0.1*sdp + 0.9*dp

9. Generator:
   z_slice [B, 192, segment_size=4096]
     â†’ Generator(z_slice, g) â†’ y_hat [B, 1, T_wav]

10. Discriminator:
    y_real [B, 1, T_wav], y_hat [B, 1, T_wav]
      â†’ MPD â†’ y_d_rs, y_d_gs, fmap_rs, fmap_gs


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TENSOR SHAPE FLOW (æ¨è«–éšæ®µ)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Inputs:
   text:       [1, T_text]
   sid:        [1] (optional)
   eid:        [1] (optional)
   ref_audio:  [1, T_wav] (optional)

2. Embeddings:
   g = emb_g(sid) + emb_e(eid): [1, gin_ch, 1]

3. Optional eGeMAPS (if ref_audio):
   ref_audio [1, T_wav]
     â†’ extractor â†’ [1, 88, T_feat]
     â†’ encoder   â†’ [1, 192, T_feat] (emo_feat)

4. Text Encoding:
   text [1, T_text]
     â†’ TextEncoder(with CCA if emo_feat)
     â†’ m_p [1, 192, T_text], logs_p [1, 192, T_text]

5. Duration Prediction:
   logw = 0.1*SDP + 0.9*DP
   w = exp(logw) * length_scale (default 1.0)
   w_ceil = ceil(w)  [1, 1, T_text]

   y_lengths = sum(w_ceil)  [1]
   y_mask = sequence_mask(y_lengths) [1, 1, T_y]

6. Alignment:
   attn = generate_path(w_ceil, attn_mask)  [1, 1, T_y, T_text]

7. Expand Features:
   m_p_expanded = attn @ m_p.T  [1, 192, T_y]
   logs_p_expanded = attn @ logs_p.T  [1, 192, T_y]

8. Sample Latent:
   z_p = m_p_expanded + randn() * exp(logs_p_expanded) * noise_scale
   z_p: [1, 192, T_y]

9. Flow Reverse:
   z = Flow^-1(z_p, g)  [1, 192, T_y]

10. Generate:
    o = Generator(z, g)  [1, 1, T_wav]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPARED TO BASELINE VITS (å°æ¯”åŸå§‹VITS)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Component | Original VITS | This Work |
|-----------|---------------|-----------|
| Emotion Control | âŒ None | âœ… eGeMAPS + CCA |
| Acoustic Features | âŒ Implicit | âœ… Explicit (88-dim) |
| Normalization | LayerNorm | âœ… Conditional LayerNorm |
| Feature Fusion | - | âœ… Cross-Conditional Attention |
| Duration Predictor | SDP or DP | âœ… SDP + DP mixed (0.1:0.9) |
| Speaker Support | âœ… Multi-speaker | âœ… Multi-speaker (58) |
| Emotion Support | âŒ None | âœ… Multi-emotion (configurable) |
| Reference Audio | âŒ Not supported | âœ… Optional for emotion transfer |
| Multi-lingual | âœ… Yes | âœ… 7 languages (HAK, TW, ZH, EN, ID, VI, JP) |

Key Additions:
  1. eGeMAPS extraction & encoding pipeline
  2. CCA module for emotion-text fusion
  3. CLN for speaker/emotion conditioning
  4. Dual duration prediction with mixing
  5. Optional reference audio for emotion transfer


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTRIBUTIONS SUMMARY (è²¢ç»ç¸½çµ)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Technical Contributions:

1. **eGeMAPS Integration in VITS**
   - First work to integrate standardized acoustic features (eGeMAPS) into VITS
   - Learnable projection from 97-dim to 88-dim
   - Encoder with residual connections

2. **Cross-Conditional Attention (CCA)**
   - Novel attention mechanism for emotion-text fusion
   - Query from text, Key/Value from emotion features
   - Preserves text content while injecting emotion

3. **Conditional Layer Normalization (CLN)**
   - Dynamic feature normalization per speaker/emotion
   - Applied in Transformer, Duration Predictor, and Decoder
   - Stable training with zero initialization

4. **Reference Audio-Driven Emotion Transfer**
   - No emotion labels required during training
   - Extract emotion from any reference audio at inference
   - Flexible emotion control

5. **Low-Resource Language TTS**
   - Designed for Taiwanese and Hakka
   - Multi-dialect support (Sixian, Hailu for Hakka)
   - Specialized phoneme processing


Application Scenarios:

- Expressive TTS for low-resource languages
- Emotion transfer without emotion-labeled data
- Cross-speaker emotion synthesis
- Audiobook narration with varying emotions
- Voice assistants with emotional responses


Future Work:

- Fine-grained emotion intensity control
- Multi-modal emotion conditioning (text + audio + facial)
- Zero-shot emotion transfer
- Real-time inference optimization
- Extension to more languages


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CODE REFERENCE TABLE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Module | File | Lines | Description |
|--------|------|-------|-------------|
| eGeMAPS Extractor | egemaps_extractor.py | 20-229 | Extract F0, Energy, MFCC, etc. |
| eGeMAPS Encoder | egemaps_extractor.py | 231-317 | Encode features to 192-dim |
| CLN | modules.py | 34-75 | Conditional LayerNorm |
| CCA | attentions.py | 257-345 | Cross-Conditional Attention |
| TextEncoder | models.py | 137-201 | With CLN & CCA |
| PosteriorEncoder | models.py | 237-266 | Training only |
| SDP | models.py | 19-97 | Stochastic Duration |
| DP | models.py | 100-134 | Deterministic Duration with CLN |
| Generator | models.py | 269-321 | HiFi-GAN vocoder |
| Discriminator | models.py | 389-411 | Multi-Period Discriminator |
| SynthesizerTrn | models.py | 415-870 | Main model |
| - forward | models.py | 512-588 | Training forward |
| - infer | models.py | 645-825 | Inference |
| Transformer Encoder | attentions.py | 13-47 | With CLN |
| Transformer Decoder | attentions.py | 50-98 | With CLN |


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Generated: 2025-12-31
Based on: VITS with eGeMAPS, CLN, and CCA
Code Version: Latest from /mnt/Linux_DATA/synthesis/model/vits/
```

---

## ğŸ“Œ é‡é»èªªæ˜

### 1. **eGeMAPS ç‰¹å¾µæå–** (æ–°å¢)

æ‚¨çš„ `egemaps_extractor.py` å¯¦ä½œäº†å®Œæ•´çš„è²å­¸ç‰¹å¾µæå–:

- **è¼¸å…¥**: åƒè€ƒéŸ³è¨Š `[B, T_wav]`
- **æå–ç‰¹å¾µ**:
  - F0 (åŸºé »)
  - Energy (èƒ½é‡)
  - MFCC (13 ç¶­)
  - Spectral Flux (é »è­œè®ŠåŒ–)
  - Zero Crossing Rate (éé›¶ç‡)
  - Mel-Spectrogram (80 ç¶­)
- **è¼¸å‡º**: 88 ç¶­å£“ç¸®ç‰¹å¾µ

### 2. **Conditional Layer Normalization** (æ–°å¢)

æ‚¨åœ¨ `modules.py:34-75` å¯¦ä½œäº† CLN:

```python
x_norm = LayerNorm(x)
if g is not None:
  gamma_c, beta_c = Conv1d(g).split()
  output = x_norm * (1 + gamma_c) + beta_c
```

**æ‡‰ç”¨ä½ç½®**:
- TextEncoder çš„ Transformer æ¯å±¤
- DurationPredictor (DP)
- Decoder çš„ Transformer æ¯å±¤

### 3. **Cross-Conditional Attention** (æ–°å¢)

æ‚¨åœ¨ `attentions.py:257-345` å¯¦ä½œäº† CCA:

- **Query**: ä¾†è‡ªæ–‡æœ¬ç·¨ç¢¼ `text_encoding`
- **Key/Value**: ä¾†è‡ªæƒ…æ„Ÿç‰¹å¾µ `emo_feat` (eGeMAPS)
- **è¼¸å‡º**: `x + CCA(x, emo_feat)` (æ®˜å·®é€£æ¥)

**ä½ç½®**: TextEncoder çš„ Transformer å±¤ä¹‹å¾Œ ([models.py:193-196](models.py:193-196))

---

## ğŸ¯ è«–æ–‡æ¶æ§‹å»ºè­°

æ ¹æ“šæ‚¨çš„å¯¦ä½œ,è«–æ–‡å¯ä»¥é€™æ¨£å¯«:

### æ¨™é¡Œå»ºè­°:
```
Emotion-Controllable Text-to-Speech via eGeMAPS Features
and Cross-Conditional Attention for Low-Resource Languages

åŸºæ–¼ eGeMAPS ç‰¹å¾µå’Œè·¨æ¢ä»¶æ³¨æ„åŠ›çš„ä½è³‡æºèªè¨€æƒ…æ„Ÿå¯æ§èªéŸ³åˆæˆ
```

### Abstract è¦é»:
1. å•é¡Œ: ä½è³‡æºèªè¨€(å°èª/å®¢èª)ç¼ºä¹æƒ…æ„Ÿæ¨™è¨»è³‡æ–™
2. æ–¹æ³•: eGeMAPS ç‰¹å¾µæå– + CCA èåˆ + CLN æ¢ä»¶åŒ–
3. è²¢ç»: åƒè€ƒéŸ³è¨Šé©…å‹•çš„æƒ…æ„Ÿé·ç§»,ç„¡éœ€æƒ…æ„Ÿæ¨™ç±¤
4. çµæœ: (å¾…å¯¦é©—æ•¸æ“š)

### æ ¸å¿ƒå‰µæ–°é»:
1. **eGeMAPS æ•´åˆ**: æ˜ç¢ºçš„è²å­¸ç‰¹å¾µå»ºæ¨¡
2. **CCA æ©Ÿåˆ¶**: éˆæ´»çš„æƒ…æ„Ÿ-æ–‡æœ¬èåˆ
3. **CLN**: å‹•æ…‹çš„ speaker/emotion æ¢ä»¶åŒ–
4. **ç„¡æ¨™ç±¤è¨“ç·´**: åƒ…éœ€éŸ³è¨Š,ç„¡éœ€æƒ…æ„Ÿæ¨™è¨»

---

éœ€è¦æˆ‘é€²ä¸€æ­¥è§£é‡‹ä»»ä½•æ¨¡çµ„æˆ–å¹«æ‚¨æº–å‚™å¯¦é©—è¨­è¨ˆå—?
