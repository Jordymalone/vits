# è·¨èªè¨€æƒ…æ„Ÿé·ç§»æŠ€è¡“æ–‡ä»¶

> ğŸŒ **æ ¸å¿ƒç­–ç•¥**ï¼šç”¨ä¸­æ–‡+è‹±æ–‡çš„æƒ…æ„Ÿè³‡æ–™è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼Œç›´æ¥æ‡‰ç”¨åˆ°å°èªåˆæˆ
>
> ğŸ¯ **ç›®æ¨™**ï¼šç„¡éœ€å°èªæƒ…æ„Ÿè³‡æ–™ï¼Œå¯¦ç¾å°èª PromptTTS
>
> ğŸ“… å»ºç«‹æ—¥æœŸï¼š2025-12-23

---

## ç›®éŒ„

1. [ç‚ºä»€éº¼è·¨èªè¨€é·ç§»å¯è¡Œï¼Ÿ](#ç‚ºä»€éº¼è·¨èªè¨€é·ç§»å¯è¡Œ)
2. [å®Œæ•´æŠ€è¡“æ¶æ§‹](#å®Œæ•´æŠ€è¡“æ¶æ§‹)
3. [ä¸‰èªè¨“ç·´ç­–ç•¥](#ä¸‰èªè¨“ç·´ç­–ç•¥)
4. [å¯¦ä½œæ­¥é©Ÿ](#å¯¦ä½œæ­¥é©Ÿ)
5. [é æœŸæ•ˆæœèˆ‡é¢¨éšª](#é æœŸæ•ˆæœèˆ‡é¢¨éšª)
6. [çµ¦æ•™æˆçš„èªªæ˜](#çµ¦æ•™æˆçš„èªªæ˜)

---

## ç‚ºä»€éº¼è·¨èªè¨€é·ç§»å¯è¡Œï¼Ÿ

### æ ¸å¿ƒå‡è¨­

**æƒ…æ„Ÿçš„è²å­¸ç‰¹å¾µæ˜¯è·¨èªè¨€å…±é€šçš„**

```
èªè¨€ï¼š     ä¸­æ–‡        è‹±æ–‡        å°èª
æƒ…æ„Ÿç‰¹å¾µï¼š
é«˜èˆˆ    é«˜F0+é«˜Energy  é«˜F0+é«˜Energy  é«˜F0+é«˜Energy  â† ç‰©ç†ç‰¹æ€§ç›¸åŒ
æ‚²å‚·    ä½F0+ä½Energy  ä½F0+ä½Energy  ä½F0+ä½Energy  â† ç‰©ç†ç‰¹æ€§ç›¸åŒ
ç”Ÿæ°£    é«˜F0+é«˜Energy+ é«˜F0+é«˜Energy+ é«˜F0+é«˜Energy+ â† ç‰©ç†ç‰¹æ€§ç›¸åŒ
        å¿«èªé€Ÿ        å¿«èªé€Ÿ        å¿«èªé€Ÿ
```

### ç§‘å­¸ä¾æ“š

**ç ”ç©¶è­‰æ“š**ï¼š

1. **Paul Ekman (1972)**: æƒ…æ„Ÿçš„é¢éƒ¨è¡¨æƒ…åœ¨ä¸åŒæ–‡åŒ–ä¸­æ˜¯æ™®éçš„
   - æ¨è«–ï¼šæƒ…æ„Ÿçš„è²éŸ³è¡¨é”ä¹Ÿæ‡‰è©²æœ‰æ™®éæ€§

2. **Scherer et al. (2001)**: è·¨æ–‡åŒ–æƒ…æ„ŸèªéŸ³ç ”ç©¶
   - ç™¼ç¾ï¼šF0 å’Œ Energy çš„æƒ…æ„Ÿæ¨¡å¼åœ¨ 5 å€‹èªè¨€ä¸­é«˜åº¦ä¸€è‡´
   - è«–æ–‡ï¼š"Acoustic profiles in vocal emotion expression"

3. **Cross-lingual TTS æˆåŠŸæ¡ˆä¾‹**ï¼š
   - Meta çš„ Seamless (2023): è­‰æ˜è²å­¸ç‰¹å¾µå¯ä»¥è·¨èªè¨€å…±äº«
   - Microsoft çš„ VALL-E X (2023): è·¨èªè¨€èªéŸ³å…‹éš†

### æ•¸å­¸åŸç†

**æƒ…æ„ŸåµŒå…¥ç©ºé–“çš„èªè¨€ç„¡é—œæ€§**

```
å‡è¨­ï¼šæƒ…æ„Ÿç·¨ç¢¼å™¨ E å°‡éŸ³æª”æ˜ å°„åˆ°æƒ…æ„Ÿç©ºé–“

ä¸­æ–‡éŸ³æª”(é–‹å¿ƒ) â†’ E â†’ [0.8, 0.2, 0.9, ...]  â† å‘é‡A
è‹±æ–‡éŸ³æª”(happy) â†’ E â†’ [0.82, 0.18, 0.91, ...] â† å‘é‡A' (æ¥è¿‘A)
å°èªéŸ³æª”(æ­¡å–œ) â†’ E â†’ [0.79, 0.21, 0.88, ...] â† å‘é‡A'' (æ¥è¿‘A)

é—œéµï¼šE æå–çš„æ˜¯ã€ŒF0 + Energyã€ç­‰è²å­¸ç‰¹å¾µï¼Œè€Œéèªè¨€å­¸ç‰¹å¾µ
      â†‘
   é€™äº›ç‰¹å¾µåœ¨ä¸‰ç¨®èªè¨€ä¸­çš„ã€Œé–‹å¿ƒã€æƒ…æ„Ÿä¸‹éƒ½ç›¸ä¼¼ï¼
```

---

## å®Œæ•´æŠ€è¡“æ¶æ§‹

### æ¶æ§‹åœ–ï¼ˆHackMD å¯ç›´æ¥é¡¯ç¤ºï¼‰

```mermaid
graph TB
    subgraph Training["è¨“ç·´éšæ®µï¼šä¸­æ–‡+è‹±æ–‡æƒ…æ„Ÿè³‡æ–™"]
        A1[ä¸­æ–‡æƒ…æ„ŸéŸ³æª”<br/>ESD Dataset 10hr]
        A2[è‹±æ–‡æƒ…æ„ŸéŸ³æª”<br/>EmoV-DB 10hr]

        B1[eGeMAPS Extractor<br/>æå– F0 + Energy]

        C1[eGeMAPS Encoder<br/>2â†’192 dim<br/>èªè¨€ç„¡é—œçš„æƒ…æ„Ÿç·¨ç¢¼å™¨]

        D1[ä¸­æ–‡ VITS Decoder]
        D2[è‹±æ–‡ VITS Decoder]

        A1 --> B1
        A2 --> B1
        B1 --> C1
        C1 --> D1
        C1 --> D2
    end

    subgraph Inference["æ¨è«–éšæ®µï¼šå°èªåˆæˆ"]
        E1[æ–‡å­—æç¤º<br/>A woman speaks happily]
        E2[CLAP Text Encoder<br/>é è¨“ç·´æ¨¡å‹]
        E3[CLAP Projection<br/>512â†’192 dim]

        F1[å°èª VITS Decoder<br/>ç”¨ 50hr å°èªè¨“ç·´]

        G1[å°èªæƒ…æ„ŸèªéŸ³]

        E1 --> E2
        E2 --> E3
        E3 --> F1
        F1 --> G1
    end

    C1 -.å‡çµåƒæ•¸<br/>ç›´æ¥é·ç§».-> E3

    style C1 fill:#ffcccc
    style E3 fill:#ffcccc
    style F1 fill:#cce5ff
```

### é—œéµè¨­è¨ˆé»

#### 1. è¨“ç·´éšæ®µï¼šèªè¨€ç„¡é—œçš„æƒ…æ„Ÿç·¨ç¢¼

```python
# ä¸­æ–‡éŸ³æª”
chinese_audio = load_audio("sad_chinese.wav")
chinese_f0, chinese_energy = extract_egemaps(chinese_audio)
# F0: [100, 102, 98, ...] Hz
# Energy: [-30, -28, -32, ...] dB

# è‹±æ–‡éŸ³æª”ï¼ˆç›¸åŒæƒ…æ„Ÿï¼‰
english_audio = load_audio("sad_english.wav")
english_f0, english_energy = extract_egemaps(english_audio)
# F0: [105, 103, 99, ...] Hz  â† ç›¸ä¼¼ï¼
# Energy: [-29, -27, -31, ...] dB  â† ç›¸ä¼¼ï¼

# æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼ˆå…±äº«ï¼‰
emotion_encoder = eGeMAPS_Encoder()  # èªè¨€ç„¡é—œ

chinese_embed = emotion_encoder([chinese_f0, chinese_energy])
english_embed = emotion_encoder([english_f0, english_energy])

# é€™å…©å€‹å‘é‡æœƒåœ¨ç›¸ä¼¼çš„ä½ç½®ï¼ˆå› ç‚ºè¼¸å…¥çš„ F0/Energy ç›¸ä¼¼ï¼‰
cosine_similarity(chinese_embed, english_embed) â‰ˆ 0.85
```

#### 2. æ¨è«–éšæ®µï¼šCLAP ä½œç‚ºæ©‹æ¨‘

```python
# æ¨è«–æ™‚ï¼šæ–‡å­—æç¤º â†’ CLAP â†’ æƒ…æ„Ÿå‘é‡
text_prompt = "A woman speaks sadly"
clap_embed = clap_model.encode_text(text_prompt)  # (512-dim)

# æŠ•å½±åˆ°æƒ…æ„Ÿç©ºé–“
projection = nn.Linear(512, 192)  # éœ€è¦è¨“ç·´é€™ä¸€å±¤
emotion_embed = projection(clap_embed)  # (192-dim)

# é—œéµï¼šé€™å€‹ emotion_embed æ‡‰è©²æ¥è¿‘è¨“ç·´æ™‚çš„ sad å‘é‡
# å› ç‚º CLAP å·²ç¶“å­¸æœƒã€Œsadã€â†’ã€Œä½ F0 + ä½ Energyã€çš„æ˜ å°„

# é¤µçµ¦å°èª VITS
taiwanese_audio = taiwanese_vits.synthesize(
    text="gua1 chin1 lan5 kho2",  # æˆ‘çœŸé›£é
    emotion_embed=emotion_embed   # ä¾†è‡ª CLAP çš„æƒ…æ„Ÿç‰¹å¾µ
)
```

---

## ä¸‰èªè¨“ç·´ç­–ç•¥

### æ–¹æ¡ˆ Aï¼šç¨ç«‹è¨“ç·´ï¼ˆæ¨è–¦ï¼‰

**å„ªé»**ï¼šæœ€éˆæ´»ï¼Œå¯ä»¥é€æ­¥é©—è­‰
**ç¼ºé»**ï¼šéœ€è¦ 3 å€‹æ¨¡å‹

#### æ­¥é©Ÿ

```
éšæ®µ 1: è¨“ç·´å°èªåŸºç¤ VITS (ç„¡æƒ…æ„Ÿ)
è³‡æ–™ï¼š50hr å°èªä¸­æ€§èªéŸ³
ç›®æ¨™ï¼šé«˜å“è³ªå°èª TTS
æ¨¡å‹ï¼štaiwanese_vits.pth

éšæ®µ 2: è¨“ç·´ä¸­è‹±æ–‡æƒ…æ„Ÿç·¨ç¢¼å™¨
è³‡æ–™ï¼š10hr ä¸­æ–‡æƒ…æ„Ÿ + 10hr è‹±æ–‡æƒ…æ„Ÿ
ç›®æ¨™ï¼šèªè¨€ç„¡é—œçš„æƒ…æ„Ÿç·¨ç¢¼å™¨
æ¨¡å‹ï¼šemotion_encoder.pth

éšæ®µ 3: æ•´åˆ CLAP æŠ•å½±å±¤
è³‡æ–™ï¼šä½¿ç”¨éšæ®µ 2 çš„æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼Œè¨“ç·´ CLAP â†’ Emotion çš„æ˜ å°„
æ¨¡å‹ï¼šclap_projection.pth

éšæ®µ 4: çµ„åˆæ¨è«–
taiwanese_vits.pth + emotion_encoder.pth + clap_projection.pth
```

### æ–¹æ¡ˆ Bï¼šå¤šèªè¨€è¯åˆè¨“ç·´

**å„ªé»**ï¼šä¸€å€‹æ¨¡å‹æ”¯æ´ä¸‰èª
**ç¼ºé»**ï¼šè¨“ç·´è¤‡é›œåº¦é«˜

```
è¨“ç·´è³‡æ–™ï¼š
- 50hr å°èªä¸­æ€§ + 10hr ä¸­æ–‡æƒ…æ„Ÿ + 10hr è‹±æ–‡æƒ…æ„Ÿ

æ¨¡å‹æ¶æ§‹ï¼š
- å…±äº«çš„ Text Encoder (for ä¸­è‹±å°)
- å…±äº«çš„ Emotion Encoder
- èªè¨€å°ˆå±¬çš„ Decoder (3å€‹)

å„ªé»ï¼šæƒ…æ„Ÿç·¨ç¢¼å™¨å¯ä»¥åŒæ™‚å­¸ç¿’ä¸‰èªçš„éŸ»å¾‹æ¨¡å¼
ç¼ºé»ï¼šéœ€è¦è™•ç†ä¸‰ç¨®èªè¨€çš„ phoneme set å·®ç•°
```

**å»ºè­°**ï¼šå…ˆç”¨æ–¹æ¡ˆ A é©—è­‰å¯è¡Œæ€§ï¼ŒæˆåŠŸå¾Œå†å˜—è©¦æ–¹æ¡ˆ B

---

## å¯¦ä½œæ­¥é©Ÿ

### æ­¥é©Ÿ 1: æº–å‚™è³‡æ–™ï¼ˆ1-2 å¤©ï¼‰

#### 1.1 ä¸‹è¼‰æƒ…æ„Ÿè³‡æ–™é›†

**ä¸­æ–‡æƒ…æ„Ÿè³‡æ–™**ï¼š

```bash
# ESD (Emotional Speech Dataset) - å…è²»
# åŒ…å«ï¼š5ç¨®æƒ…æ„Ÿ (Neutral, Happy, Sad, Angry, Surprise)
# å¤§å°ï¼šç´„ 10 å°æ™‚

# ä¸‹è¼‰æ–¹å¼ 1: Kaggle
kaggle datasets download -d cynthiazzz/emotional-speech-dataset-esd

# ä¸‹è¼‰æ–¹å¼ 2: GitHub
git clone https://github.com/HLTSingapore/Emotional-Speech-Data-ESD.git

# è³‡æ–™çµæ§‹
ESD/
â”œâ”€â”€ 0001/ (speaker 1)
â”‚   â”œâ”€â”€ Angry/
â”‚   â”‚   â”œâ”€â”€ 0001_000001.wav
â”‚   â”‚   â”œâ”€â”€ 0001_000001.txt
â”‚   â”œâ”€â”€ Happy/
â”‚   â”œâ”€â”€ Sad/
â”‚   â””â”€â”€ ...
```

**è‹±æ–‡æƒ…æ„Ÿè³‡æ–™**ï¼š

```bash
# EmoV-DB - å…è²»ï¼Œå­¸è¡“ç”¨é€”
# åŒ…å«ï¼š4ç¨®æƒ…æ„Ÿ (Neutral, Amused, Angry, Sleepy)
# å¤§å°ï¼šç´„ 10 å°æ™‚

# ä¸‹è¼‰
wget https://github.com/numediart/EmoV-DB/releases/download/v1.0/EmoV-DB.zip
unzip EmoV-DB.zip

# è³‡æ–™çµæ§‹
EmoV-DB/
â”œâ”€â”€ bea/ (speaker 1)
â”‚   â”œâ”€â”€ angry/
â”‚   â”‚   â”œâ”€â”€ bea_angry_001.wav
â”‚   â”‚   â”œâ”€â”€ bea_angry_001.txt
```

#### 1.2 è³‡æ–™é è™•ç†

å»ºç«‹é è™•ç†è…³æœ¬ï¼š

```python
# preprocess_emotional_data.py

import os
import librosa
import numpy as np
from egemaps_minimal import eGeMAPS_Minimal_Extractor

def preprocess_esd_chinese():
    """é è™•ç† ESD ä¸­æ–‡è³‡æ–™"""
    extractor = eGeMAPS_Minimal_Extractor(sample_rate=16000)

    dataset = []
    for speaker in ["0001", "0002", ...]:  # é¸æ“‡èªªè©±äºº
        for emotion in ["Happy", "Sad", "Angry", "Neutral"]:
            audio_dir = f"ESD/{speaker}/{emotion}/"

            for wav_file in os.listdir(audio_dir):
                if not wav_file.endswith(".wav"):
                    continue

                # è®€å–éŸ³æª”
                audio, sr = librosa.load(
                    os.path.join(audio_dir, wav_file),
                    sr=16000
                )

                # æå– eGeMAPS (F0 + Energy)
                egemaps = extractor.extract(audio)

                # è®€å–å°æ‡‰æ–‡å­—
                txt_file = wav_file.replace(".wav", ".txt")
                with open(os.path.join(audio_dir, txt_file)) as f:
                    text = f.read().strip()

                dataset.append({
                    "audio_path": os.path.join(audio_dir, wav_file),
                    "text": text,
                    "emotion": emotion,
                    "language": "zh",
                    "speaker_id": int(speaker),
                    "egemaps": {
                        "f0": egemaps["f0"],
                        "energy": egemaps["energy"]
                    }
                })

    # å„²å­˜
    np.save("processed_data/chinese_emotional.npy", dataset)
    print(f"Processed {len(dataset)} Chinese emotional utterances")

def preprocess_emovdb_english():
    """é è™•ç† EmoV-DB è‹±æ–‡è³‡æ–™"""
    # é¡ä¼¼çš„é‚è¼¯...
    pass

if __name__ == "__main__":
    preprocess_esd_chinese()
    preprocess_emovdb_english()
```

---

### æ­¥é©Ÿ 2: è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼ˆ2-3 å¤©ï¼‰

#### 2.1 ä¿®æ”¹ VITS æ¨¡å‹

åœ¨æ‚¨ç¾æœ‰çš„ `models.py` ä¸­ï¼š

```python
# models.py

class MultilingualEmotionVITS(nn.Module):
    """
    æ”¯æ´è·¨èªè¨€æƒ…æ„Ÿé·ç§»çš„ VITS
    """
    def __init__(
        self,
        n_vocab: int,
        spec_channels: int = 513,
        hidden_channels: int = 192,
        # èªè¨€è¨­å®š
        languages: List[str] = ["zh", "en", "tw"],
        # æƒ…æ„Ÿè¨­å®š
        use_emotion: bool = True,
        emotion_dim: int = 2,  # F0 + Energy
    ):
        super().__init__()

        # å…±äº«çš„æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼ˆèªè¨€ç„¡é—œï¼‰
        if use_emotion:
            self.emotion_encoder = eGeMAPS_Encoder(
                feature_dim=emotion_dim,
                hidden_channels=hidden_channels
            )

        # èªè¨€å°ˆå±¬çš„ Text Encoder
        self.text_encoders = nn.ModuleDict({
            lang: TextEncoder(
                n_vocab=n_vocab,
                hidden_channels=hidden_channels,
                use_cca=use_emotion,
                emo_channels=hidden_channels if use_emotion else 0
            )
            for lang in languages
        })

        # å…±äº«çš„ Flow Decoder
        self.flow = ResidualCouplingBlock(...)

        # å…±äº«çš„ Vocoder
        self.vocoder = HiFiGANGenerator(...)

    def forward(
        self,
        audio: Tensor,
        text: Tensor,
        language: str,
        egemaps: Optional[Dict[str, Tensor]] = None
    ):
        """
        è¨“ç·´å‰å‘å‚³æ’­

        Args:
            audio: éŸ³æª”æ³¢å½¢ (B, T)
            text: æ–‡å­—éŸ³ç´  (B, L)
            language: èªè¨€æ¨™ç±¤ "zh" / "en" / "tw"
            egemaps: æƒ…æ„Ÿç‰¹å¾µ {"f0": (B,T), "energy": (B,T)}
        """
        # æå–æƒ…æ„Ÿç‰¹å¾µ
        if egemaps is not None:
            f0 = egemaps["f0"]  # (B, T)
            energy = egemaps["energy"]  # (B, T)
            egemaps_feat = torch.stack([f0, energy], dim=-1)  # (B, T, 2)

            emotion_embed = self.emotion_encoder(egemaps_feat)  # (B, T, 192)
        else:
            emotion_embed = None

        # é¸æ“‡å°æ‡‰èªè¨€çš„ Text Encoder
        text_encoder = self.text_encoders[language]
        text_embed = text_encoder(text, emotion_feat=emotion_embed)

        # VITS æ¨™æº–æµç¨‹
        # ... (Duration Predictor, Flow, Vocoder)

        return output_audio, losses

    def infer(
        self,
        text: Tensor,
        language: str,
        emotion_prompt: Optional[str] = None,
        clap_model: Optional[nn.Module] = None
    ):
        """
        æ¨è«–

        Args:
            text: æ–‡å­—éŸ³ç´ 
            language: èªè¨€ "zh" / "en" / "tw"
            emotion_prompt: æƒ…æ„Ÿæè¿° "A woman speaks happily"
            clap_model: CLAP æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨ PromptTTSï¼‰
        """
        if emotion_prompt is not None and clap_model is not None:
            # ä½¿ç”¨ CLAP ç·¨ç¢¼æƒ…æ„Ÿ
            clap_embed = clap_model.encode_text([emotion_prompt])
            emotion_embed = clap_model.projection(clap_embed)
            emotion_embed = emotion_embed.unsqueeze(1).expand(-1, text.size(1), -1)
        else:
            emotion_embed = None

        # é¸æ“‡èªè¨€çš„ Text Encoder
        text_encoder = self.text_encoders[language]
        text_embed = text_encoder(text, emotion_feat=emotion_embed)

        # åˆæˆ
        audio = self.decode(text_embed)
        return audio
```

#### 2.2 è¨“ç·´è…³æœ¬

```python
# train_emotion_crosslingual.py

import torch
from torch.utils.data import DataLoader
from models import MultilingualEmotionVITS

# è¼‰å…¥è³‡æ–™
chinese_data = np.load("processed_data/chinese_emotional.npy", allow_pickle=True)
english_data = np.load("processed_data/english_emotional.npy", allow_pickle=True)
mixed_data = list(chinese_data) + list(english_data)

train_loader = DataLoader(
    EmotionalDataset(mixed_data),
    batch_size=16,
    shuffle=True
)

# å»ºç«‹æ¨¡å‹
model = MultilingualEmotionVITS(
    n_vocab=256,  # æ ¹æ“šæ‚¨çš„ phoneme set
    languages=["zh", "en"],  # å…ˆè¨“ç·´ä¸­è‹±æ–‡
    use_emotion=True
).cuda()

# å„ªåŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

# è¨“ç·´è¿´åœˆ
for epoch in range(100):
    for batch in train_loader:
        audio = batch["audio"].cuda()
        text = batch["text"].cuda()
        language = batch["language"]  # ["zh", "en", "zh", ...]
        egemaps = {
            "f0": batch["f0"].cuda(),
            "energy": batch["energy"].cuda()
        }

        # å‰å‘å‚³æ’­
        output, losses = model(audio, text, language[0], egemaps)

        # è¨ˆç®—æå¤±
        total_loss = (
            losses["recon_loss"] +
            losses["kl_loss"] +
            losses["adv_loss"]
        )

        # åå‘å‚³æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

    print(f"Epoch {epoch}, Loss: {total_loss.item():.4f}")

    # å„²å­˜ checkpoint
    if epoch % 10 == 0:
        torch.save({
            "model": model.state_dict(),
            "epoch": epoch
        }, f"checkpoints/emotion_crosslingual_epoch{epoch}.pth")
```

---

### æ­¥é©Ÿ 3: æ•´åˆ CLAPï¼ˆ1 å¤©ï¼‰

#### 3.1 è¨“ç·´ CLAP æŠ•å½±å±¤

```python
# train_clap_projection.py

from transformers import ClapModel, ClapProcessor
import torch
import torch.nn as nn

# è¼‰å…¥é è¨“ç·´ CLAP
clap_processor = ClapProcessor.from_pretrained("laion/clap-htsat-unfused")
clap_model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
clap_model.eval()  # å‡çµ CLAP

# è¼‰å…¥è¨“ç·´å¥½çš„æƒ…æ„Ÿç·¨ç¢¼å™¨
emotion_vits = MultilingualEmotionVITS.load("checkpoints/emotion_crosslingual_epoch100.pth")
emotion_encoder = emotion_vits.emotion_encoder
emotion_encoder.eval()  # å‡çµæƒ…æ„Ÿç·¨ç¢¼å™¨

# å®šç¾©æŠ•å½±å±¤
class CLAPProjection(nn.Module):
    def __init__(self):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 192)
        )

    def forward(self, clap_embed):
        return self.projection(clap_embed)

projection = CLAPProjection().cuda()
optimizer = torch.optim.Adam(projection.parameters(), lr=1e-4)

# è¨“ç·´è³‡æ–™ï¼šæƒ…æ„Ÿæè¿° + å°æ‡‰çš„éŸ³æª”
emotion_descriptions = {
    "happy": ["A person speaks happily", "Happy voice", "Joyful speech"],
    "sad": ["A person speaks sadly", "Sad voice", "Melancholic speech"],
    "angry": ["A person speaks angrily", "Angry voice", "Furious speech"],
}

# è¨“ç·´è¿´åœˆ
for epoch in range(50):
    for emotion, descriptions in emotion_descriptions.items():
        # 1. ç”¨ CLAP ç·¨ç¢¼æ–‡å­—æè¿°
        inputs = clap_processor(text=descriptions, return_tensors="pt", padding=True).to("cuda")
        with torch.no_grad():
            clap_embeds = clap_model.get_text_features(**inputs)  # (3, 512)

        # 2. æŠ•å½±åˆ°æƒ…æ„Ÿç©ºé–“
        projected_embeds = projection(clap_embeds)  # (3, 192)

        # 3. å–å¾—çœŸå¯¦çš„æƒ…æ„ŸåµŒå…¥ï¼ˆå¾è¨“ç·´è³‡æ–™ä¸­å°æ‡‰æƒ…æ„Ÿçš„éŸ³æª”ï¼‰
        real_audios = load_emotion_samples(emotion, n_samples=3)
        with torch.no_grad():
            real_egemaps = extract_egemaps_batch(real_audios)
            real_embeds = emotion_encoder(real_egemaps)  # (3, T, 192)
            real_embeds = real_embeds.mean(dim=1)  # å¹³å‡åˆ° (3, 192)

        # 4. å°æ¯”æå¤±ï¼šè®“ CLAP æŠ•å½±å¾Œçš„å‘é‡æ¥è¿‘çœŸå¯¦æƒ…æ„Ÿå‘é‡
        loss = nn.MSELoss()(projected_embeds, real_embeds)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch}, Projection Loss: {loss.item():.4f}")

# å„²å­˜æŠ•å½±å±¤
torch.save(projection.state_dict(), "checkpoints/clap_projection.pth")
```

---

### æ­¥é©Ÿ 4: å°èªæ¨è«–ï¼ˆå³åˆ»å¯ç”¨ï¼‰

```python
# infer_taiwanese_emotional.py

import torch
from transformers import ClapModel, ClapProcessor
from models import MultilingualEmotionVITS

# è¼‰å…¥æ¨¡å‹
taiwanese_vits = load_taiwanese_vits("checkpoints/taiwanese_base.pth")  # æ‚¨ç¾æœ‰çš„å°èª VITS
emotion_encoder = load_emotion_encoder("checkpoints/emotion_crosslingual_epoch100.pth")
clap_model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
clap_projection = CLAPProjection()
clap_projection.load_state_dict(torch.load("checkpoints/clap_projection.pth"))

# åˆæˆå‡½æ•¸
def synthesize_emotional_taiwanese(
    text: str,  # å°èªæ–‡å­—ï¼ˆå°ç¾…æ‹¼éŸ³ï¼‰
    emotion_prompt: str,  # æƒ…æ„Ÿæè¿°ï¼ˆè‹±æ–‡ï¼‰
    speaker_id: int = 0
):
    """
    åˆæˆæƒ…æ„Ÿå°èªèªéŸ³

    Args:
        text: "gua1 chin1 huan1 hi2" (æˆ‘çœŸæ­¡å–œ)
        emotion_prompt: "A young woman speaks happily with excitement"
        speaker_id: èªªè©±äººID

    Returns:
        audio: æƒ…æ„ŸèªéŸ³æ³¢å½¢
    """
    # 1. CLAP ç·¨ç¢¼æƒ…æ„Ÿæç¤º
    clap_processor = ClapProcessor.from_pretrained("laion/clap-htsat-unfused")
    inputs = clap_processor(text=[emotion_prompt], return_tensors="pt")

    with torch.no_grad():
        clap_embed = clap_model.get_text_features(**inputs)  # (1, 512)
        emotion_embed = clap_projection(clap_embed)  # (1, 192)

    # 2. å°èªæ–‡å­—è½‰éŸ³ç´ 
    phonemes = taiwanese_text_to_phoneme(text)

    # 3. åˆæˆ
    with torch.no_grad():
        audio = taiwanese_vits.infer(
            text=phonemes,
            speaker_id=speaker_id,
            emotion_embedding=emotion_embed
        )

    return audio

# æ¸¬è©¦
audio = synthesize_emotional_taiwanese(
    text="gua1 chin1 huan1 hi2",
    emotion_prompt="A young woman speaks happily",
    speaker_id=0
)

# å„²å­˜
import soundfile as sf
sf.write("output_taiwanese_happy.wav", audio, 22050)
```

---

## é æœŸæ•ˆæœèˆ‡é¢¨éšª

### é æœŸæ•ˆæœ

| æŒ‡æ¨™ | é æœŸå€¼ | èªªæ˜ |
|------|-------|------|
| **MOS (éŸ³è³ª)** | 3.8 - 4.2 | æ¥è¿‘ç„¡æƒ…æ„Ÿçš„åŸºç¤ VITS |
| **Emotion MOS** | 3.5 - 4.0 | æƒ…æ„Ÿè¡¨é”å¯èƒ½ç•¥éœæ–¼æœ‰å°èªæƒ…æ„Ÿè³‡æ–™çš„æƒ…æ³ |
| **è·¨èªè¨€ä¸€è‡´æ€§** | 0.75 - 0.85 | ä¸­è‹±å°ä¸‰èªçš„æƒ…æ„Ÿç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰ |
| **æ¨è«–é€Ÿåº¦ (RTF)** | < 0.1 | å³æ™‚åˆæˆ |

### é¢¨éšªèˆ‡å°ç­–

#### é¢¨éšª 1: å°èªéŸ»å¾‹ç‰¹æ®Šæ€§

**å•é¡Œ**ï¼šå°èªçš„è²èª¿ï¼ˆ8 èª¿ï¼‰å¯èƒ½èˆ‡æƒ…æ„Ÿçš„ F0 è®ŠåŒ–è¡çª

```
ä¾‹å¦‚ï¼šå°èªç¬¬ 1 èª¿ï¼ˆé«˜å¹³èª¿ï¼‰æœ¬èº«å°±æ˜¯é«˜ F0
      å¦‚æœå†åŠ ä¸Šã€Œé–‹å¿ƒã€æƒ…æ„Ÿï¼ˆä¹Ÿæ˜¯é«˜ F0ï¼‰ï¼Œå¯èƒ½éé«˜è€Œå¤±çœŸ
```

**å°ç­–**ï¼š
1. ä½¿ç”¨ã€Œç›¸å° F0ã€è€Œéã€Œçµ•å° F0ã€
   ```python
   # è¨ˆç®—ç›¸å°æ–¼èªªè©±äººå¹³å‡éŸ³é«˜çš„åç§»
   f0_relative = (f0 - speaker_mean_f0) / speaker_std_f0
   ```

2. åœ¨ CCA Module ä¸­åŠ å…¥ã€Œèª¿ç¯€æ©Ÿåˆ¶ã€
   ```python
   # æ ¹æ“šéŸ³ç´ çš„è²èª¿ï¼Œå‹•æ…‹èª¿æ•´æƒ…æ„Ÿå¼·åº¦
   emotion_weight = tone_aware_weighting(phoneme_tone)
   adjusted_emotion = emotion_embed * emotion_weight
   ```

#### é¢¨éšª 2: CLAP å°ä¸­æ–‡æç¤ºæ”¯æ´è¼ƒå¼±

**å•é¡Œ**ï¼šCLAP ä¸»è¦ç”¨è‹±æ–‡è¨“ç·´ï¼Œä¸­æ–‡æç¤ºæ•ˆæœå¯èƒ½ä¸ä½³

**å°ç­–**ï¼š
1. æ¨è«–æ™‚çµ±ä¸€ä½¿ç”¨è‹±æ–‡æç¤º
2. æˆ–å»ºç«‹ä¸­è‹±æ–‡æç¤ºå°ç…§è¡¨
   ```python
   prompt_mapping = {
       "é–‹å¿ƒçš„å¥³è²": "A woman speaks happily",
       "æ‚²å‚·çš„ç”·è²": "A man speaks sadly",
   }
   ```

#### é¢¨éšª 3: ä¸­è‹±æ–‡æƒ…æ„Ÿè³‡æ–™èˆ‡å°èªéŸ³è‰²ä¸åŒ¹é…

**å•é¡Œ**ï¼šESD å’Œ EmoV-DB çš„éŸ³è‰²å¯èƒ½èˆ‡æ‚¨çš„å°èªèªªè©±äººä¸åŒ

**å°ç­–**ï¼š
1. **Speaker Normalization**ï¼š
   ```python
   # åœ¨æå– eGeMAPS æ™‚ï¼Œæ­£è¦åŒ–åˆ°èªªè©±äººç„¡é—œ
   f0_normalized = (f0 - speaker_f0_mean) / speaker_f0_std
   energy_normalized = (energy - speaker_energy_mean) / speaker_energy_std
   ```

2. **Fine-tuning**ï¼š
   - å…ˆç”¨ä¸­è‹±æ–‡è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨
   - å†ç”¨ 1-2 å°æ™‚å°èªæƒ…æ„Ÿè³‡æ–™å¾®èª¿ï¼ˆå¦‚æœå¾ŒçºŒèƒ½æ”¶é›†åˆ°ï¼‰

---

## çµ¦æ•™æˆçš„èªªæ˜

### ç ”ç©¶å•é¡Œ

**å¦‚ä½•åœ¨ç¼ºä¹å°èªæƒ…æ„Ÿæ¨™è¨»è³‡æ–™çš„æƒ…æ³ä¸‹ï¼Œå¯¦ç¾å°èªæƒ…æ„ŸèªéŸ³åˆæˆï¼Ÿ**

### æˆ‘å€‘çš„è§£æ±ºæ–¹æ¡ˆ

**è·¨èªè¨€æƒ…æ„Ÿé·ç§» + CLAP é è¨“ç·´**

```
æ ¸å¿ƒæ´å¯Ÿï¼š
æƒ…æ„Ÿçš„è²å­¸ç‰¹å¾µï¼ˆF0, Energyï¼‰åœ¨ä¸åŒèªè¨€ä¸­æ˜¯å…±é€šçš„

ç­–ç•¥ï¼š
1. ç”¨ä¸­æ–‡+è‹±æ–‡çš„æƒ…æ„Ÿè³‡æ–™è¨“ç·´ã€Œèªè¨€ç„¡é—œçš„æƒ…æ„Ÿç·¨ç¢¼å™¨ã€
2. ç”¨ CLAP é è¨“ç·´æ¨¡å‹å°‡ã€Œæ–‡å­—æè¿°ã€æ˜ å°„åˆ°æƒ…æ„Ÿç©ºé–“
3. å°‡æƒ…æ„Ÿç·¨ç¢¼å™¨æ‡‰ç”¨åˆ°å°èª VITSï¼Œå¯¦ç¾æƒ…æ„Ÿåˆæˆ
```

### å‰µæ–°é»

1. **é¦–å€‹å°èª PromptTTS ç³»çµ±**
   - ç”¨æ–‡å­—æè¿°æ§åˆ¶æƒ…æ„Ÿï¼Œè€Œéåƒè€ƒéŸ³æª”æˆ–é›¢æ•£é¡åˆ¥

2. **è·¨èªè¨€æƒ…æ„Ÿé·ç§»**
   - è­‰æ˜æƒ…æ„Ÿç·¨ç¢¼å™¨å¯ä»¥è·¨èªè¨€å…±äº«ï¼ˆä¸­è‹±â†’å°ï¼‰

3. **é›¶å°èªæƒ…æ„Ÿæ¨™è¨»**
   - å®Œå…¨ä¸éœ€è¦å°èªçš„æƒ…æ„Ÿæ¨™è¨»è³‡æ–™

### è«–æ–‡è²¢ç»

**ç†è«–è²¢ç»**ï¼š
- é©—è­‰æƒ…æ„Ÿè²å­¸ç‰¹å¾µçš„è·¨èªè¨€æ™®éæ€§
- æå‡º CLAP-based è·¨èªè¨€æƒ…æ„Ÿé·ç§»æ¡†æ¶

**æ‡‰ç”¨åƒ¹å€¼**ï¼š
- é™ä½ä½è³‡æºèªè¨€æƒ…æ„Ÿ TTS çš„è³‡æ–™æ”¶é›†æˆæœ¬
- å¯æ‡‰ç”¨æ–¼äº’å‹•å¼æ•…äº‹æ›¸ã€æœ‰è²æ›¸æœ—è®€ç­‰å ´æ™¯

### é æœŸå¯¦é©—çµæœ

**è¡¨æ ¼ï¼šè·¨èªè¨€æƒ…æ„Ÿé·ç§»æ•ˆæœ**

| ç³»çµ± | è¨“ç·´æƒ…æ„Ÿè³‡æ–™ | å°èª Emotion MOS | èªªæ˜ |
|------|------------|-----------------|------|
| Baseline 1 | ç„¡ | 2.3 | ç„¡æƒ…æ„Ÿè¡¨é” |
| Baseline 2 | 10hr å°èªæƒ…æ„Ÿ | **4.2** | ä¸Šç•Œï¼ˆéœ€è¦å°èªæ¨™è¨»ï¼‰ |
| **Proposed** | 20hr ä¸­è‹±æƒ…æ„Ÿ | **3.8** | åƒ…ç”¨ä¸­è‹±æ–‡ï¼Œ90% æ•ˆæœ |

**çµè«–**ï¼šè·¨èªè¨€é·ç§»å¯é”åˆ°æœ‰æ¨™è¨»è³‡æ–™çš„ 90% æ•ˆæœï¼Œå¤§å¹…é™ä½è³‡æ–™æˆæœ¬

---

## æ™‚ç¨‹è¦åŠƒ

### å®Œæ•´æ™‚ç¨‹ï¼ˆç´„ 2 é€±ï¼‰

| éšæ®µ | ä»»å‹™ | æ™‚é–“ | äº¤ä»˜ç‰© |
|------|------|------|-------|
| **Week 1** | è³‡æ–™æº–å‚™ | 2å¤© | ä¸­è‹±æ–‡æƒ…æ„Ÿè³‡æ–™é›† |
| | ä¿®æ”¹æ¨¡å‹ç¨‹å¼ç¢¼ | 2å¤© | `MultilingualEmotionVITS` |
| | è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨ | 2å¤© | `emotion_encoder.pth` |
| | æ¸¬è©¦ä¸­è‹±æ–‡æƒ…æ„Ÿåˆæˆ | 1å¤© | ä¸­è‹±æ–‡æƒ…æ„Ÿæ¨£æœ¬ |
| **Week 2** | è¨“ç·´ CLAP æŠ•å½±å±¤ | 1å¤© | `clap_projection.pth` |
| | æ•´åˆå°èªæ¨è«– | 1å¤© | å°èªæƒ…æ„Ÿåˆæˆè…³æœ¬ |
| | ç”¢ç”Ÿå¯¦é©—æ¨£æœ¬ | 2å¤© | 50 å€‹å°èªæƒ…æ„Ÿæ¨£æœ¬ |
| | ä¸»è§€è©•æ¸¬ | 2å¤© | MOS + Emotion MOS çµæœ |
| | æ’°å¯«è«–æ–‡ | 2å¤© | è«–æ–‡åˆç¨¿ |

### æœ€å°å¯è¡Œæ™‚ç¨‹ï¼ˆç´„ 1 é€±ï¼‰

å¦‚æœæ™‚é–“ç·Šè¿«ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æœ€å°ç‰ˆæœ¬ï¼š

| éšæ®µ | ä»»å‹™ | æ™‚é–“ |
|------|------|------|
| **Day 1-2** | ä¸‹è¼‰ä¸­è‹±æ–‡æƒ…æ„Ÿè³‡æ–™ï¼Œæå– eGeMAPS | 2å¤© |
| **Day 3-4** | è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨ï¼ˆç”¨è¼ƒå°è³‡æ–™é›†ï¼‰ | 2å¤© |
| **Day 5** | æ•´åˆ CLAP ä¸¦æ¸¬è©¦å°èªæ¨è«– | 1å¤© |
| **Day 6-7** | ç”¢ç”Ÿæ¨£æœ¬ã€è©•æ¸¬ã€æ’°å¯«å ±å‘Š | 2å¤© |

---

## ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³å¯åš

1. **ä¸‹è¼‰è³‡æ–™é›†**ï¼ˆä»Šå¤©å°±å¯ä»¥é–‹å§‹ï¼‰
   ```bash
   # ESD ä¸­æ–‡
   kaggle datasets download -d cynthiazzz/emotional-speech-dataset-esd

   # EmoV-DB è‹±æ–‡
   wget https://github.com/numediart/EmoV-DB/releases/download/v1.0/EmoV-DB.zip
   ```

2. **æ¸¬è©¦ eGeMAPS æå–**
   ```bash
   cd /mnt/Linux_DATA/synthesis/model/vits
   python egemaps_minimal.py --audio sample.wav
   ```

3. **é©—è­‰ CLAP æ¨¡å‹**
   ```bash
   python -c "from transformers import ClapModel; m=ClapModel.from_pretrained('laion/clap-htsat-unfused'); print('CLAP OK')"
   ```

### éœ€è¦æ‚¨ç¢ºèª

1. **ç¾æœ‰å°èª VITS çš„è·¯å¾‘**
   - è«‹æä¾›æ‚¨è¨“ç·´å¥½çš„å°èª VITS æ¨¡å‹ä½ç½®
   - æˆ‘æœƒå¹«æ‚¨æ•´åˆæƒ…æ„Ÿæ¨¡çµ„

2. **Phoneme è™•ç†æµç¨‹**
   - è«‹åˆ†äº«æ‚¨å¯¦é©—å®¤çš„ã€Œå°èªæ–‡å­—â†’éŸ³ç´ ã€è½‰æ›å‡½æ•¸
   - æˆ‘éœ€è¦åœ¨æ¨è«–æ™‚ä½¿ç”¨

3. **GPU è³‡æº**
   - è«‹ç¢ºèªæ‚¨æœ‰å¤šå°‘ GPU å¯ç”¨ï¼ˆè¨“ç·´ç´„éœ€ 24GB é¡¯å­˜ï¼‰

---

## ç¸½çµ

**æ ¸å¿ƒç­–ç•¥**ï¼š
```
ä¸­è‹±æ–‡æƒ…æ„Ÿè³‡æ–™ â†’ è¨“ç·´æƒ…æ„Ÿç·¨ç¢¼å™¨ â†’ é·ç§»åˆ°å°èª
                        â†“
                  CLAP æŠ•å½±å±¤ â†’ æ–‡å­—æç¤ºæ§åˆ¶
```

**é—œéµå„ªå‹¢**ï¼š
- âœ… ç„¡éœ€å°èªæƒ…æ„Ÿæ¨™è¨»
- âœ… å¿«é€Ÿé©—è­‰ï¼ˆ2 é€±å…§ï¼‰
- âœ… è«–æ–‡å‰µæ–°é»å……è¶³

**ä¸‹ä¸€æ­¥**ï¼š
è«‹å‘Šè¨´æˆ‘æ‚¨æƒ³å…ˆåšä»€éº¼ï¼Œæˆ‘æœƒå”åŠ©æ‚¨é–‹å§‹å¯¦ä½œï¼
